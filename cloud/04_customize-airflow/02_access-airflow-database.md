---
title: "The Airflow Database"
navtitle: "The Airflow Database"
description: "How to access Airflow's Postgres Metadata Database on Astronomer."
---


## Overview

On Astronomer, each Airflow deployment is equipped with a PostgreSQL database that serves as Airflow's underlying metadata database and your Airflow scheduler's source of truth.

Astronomer hosts that database for Astronomer Cloud customers (each securely isolated from the rest) and our team helps Enterprise customers set up their own during our platform installation.

This guide will cover guidelines for the following:

- Risks associated with and use cases for Database Access
- Access to Airflow's Database in Local Development
- Access to Airflow's Database on Astronomer
- Example DAG that incorporates a query to the database

> Note: Airflow's "Ad-Hoc Query" feature used to be a common way to test DB connections and query the Airflow Metadata Database via the Airflow UI but was recently deprecated in 1.10 for security reasons.

### Risk Associated with Database Access

As noted above, your Airflow Deployment's Metadata Database on Astronomer is either hosted by your team if you're using Astronomer Enterprise or by ours if you're running on Astronomer Cloud. In both cases, it's worth noting the risks associated with accessing Airflow's Database given its importance to the Scheduler's performance.

We _strongly_ recommend users do not write to the database directly as it can compromise both the integrity of your Airflow Deployment and our team's ability to support you in the case of an issue.

### Use Cases

Keeping the above risk in mind, pulling from and reading the database safely can be a great way to gather metadata from your Airflow Deployment that isn't otherwise exposed on Astronomer and Airflow (yet). 

A few examples of what you can query for:
- Completed Task Count
- Tasks started per hour, per week
- Task duration

Read below for DB access guidelines both locally and on Astronomer.

## Access to Airflow's Database in Local Development

To successfully query from your Airflow Deployment's Database, you'll need to set up your local Postgres connection.

**1. Find the `airflow_db` Connection Object**

On the Airflow UI, navigate over to `Admin` > `Connections`.

The `airflow_db` connection is generated by default.

![Connections Page](https://assets2.astronomer.io/main/docs/query-postgres/query-postgres-connection-page.png)

**2. Edit the Connection**

In the `airflow_db` connection object:

- Change the `Conn Type` to `Postgres`
- Add the following connection information:

    ```
    host: postgres
    schema: postgres
    login: postgres
    password: postgres
    port (optional): 5432
    ```
Your connection should look something like:

![Local Connections Details Page](https://assets2.astronomer.io/main/docs/query-postgres/query-postgres-connection-details.png)

The port will be set to 5342 by default but if you’ve set a custom port, you’ll need to update that here as well.

**3. Connect to the DB via a PostgreSQL Client**

With the connection information above, you should be able to connect to the Airflow database from any PostgreSQL client.

### Access to Airflow's Database on Astronomer

The easiest way to pull from Airflow's Metadata Database on Astronomer is to leverage the `AIRFLOW_CONN_AIRFLOW_DB` Environment Variable, which we set [here](https://github.com/astronomer/airflow-chart/blob/master/templates/_helpers.yaml#L16-L20).

This Environment Variable, which we set by default, silently enables users to leverage the `airflow_db` connection. It's worth noting that the connection itself in the Airflow UI will NOT reflect the correct credentials (Conn Type, Host, Schema, Login, Password, Port).

To pull from the Airflow Database, follow the steps below. Note that you do _not_ have to set this Environment Variable yourself and you do _not_ have to populate the `airflow_db` connection in the Airflow UI.

#### 1. Leave your `airflow_db` Connection as is

Your `airflow_db` connection by default will look like the following:

```
Conn Id: airflow_db
Conn Type: MySQL
Host: mysql
Schema: airflow
Login: root
```

While this information is incorrect in the Airflow UI, the underlying connection will still succeed, as connections set by an Environment Variable take precedence over connection details in the Astronomer UI and in the Metadata Database itself.

For clarity, we intend to make sure this connection is properly populated in upcoming versions of Astronomer.

#### 2. Call `airflow_db` in your DAG

Here's an example DAG, where `postgres_conn_id` is set to `airflow_db`:

```py
from airflow import DAG
from airflow.hooks.postgres_hook import PostgresHook
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta


def get_dag_ids():
    postgres_hook = PostgresHook(postgres_conn_id="airflow_db")
    records = postgres_hook.get_records(sql="select dag_id from dag")
    print(records)


with DAG(
    "example_dag",
    start_date=datetime(2019, 1, 1),
    max_active_runs=1,
    schedule_interval=None,
    # catchup=False # enable if you don't want historical dag runs to run
) as dag:

    t1 = PythonOperator(
        task_id="get_dag_nums",
        python_callable=get_dag_ids,
        sql="select dag_id from dag",
        postgres_conn_id="airflow_db",
    )
```

Here, the task above grabs all `dag_ids` stored inside `dag` Table within Airflow's Metadata Database.

#### 3. Verify the Connection in your Task Logs

To verify a successful connection, you can inspect the corresponding task log -

```
Dependencies all met for <TaskInstance: example_dag.get_dag_nums 2020-05-07T19:02:38.022685+00:00 [queued]>

--------------------------------------------------------------------------------
Starting attempt 1 of 1

--------------------------------------------------------------------------------
Executing <Task(PythonOperator): get_dag_nums> on 2020-05-07T19:02:38.022685+00:00
Started process 429 to run task
Running %s on host %s <TaskInstance: example_dag.get_dag_nums 2020-05-07T19:02:38.022685+00:00 [running]> desolate-spectrum-3088-worker-866745d995-rn576
[2020-05-07 19:02:54,033] {base_hook.py:87} INFO - Using connection to: id: airflow_db. Host: desolate-spectrum-3088-pgbouncer.astronomer-desolate-spectrum-3088.svc.cluster.local, Port: 6543, Schema: desolate-spectrum-3088-metadata, Login: desolate_spectrum_3088_airflow, Password: XXXXXXXX, extra: XXXXXXXX
[('example_dag',)]
Done. Returned value was: None
```

In these logs, you can see that the connection info is properly passed:

```
Conn Id: airflow_db
Host: desolate-spectrum-3088-pgbouncer
Schema: desolate-spectrum-3088-metadata
Login: desolate_spectrum_3088_airflow
Password: XXXXXXXX
Port: 6543
```

### Populating the `airflow_db` Connection on Astronomer Enterprise

On Astronomer, your deployment's Postgres credentials are also stored as a Kubernetes Secret in your deployment's namespace. For users with `kubectl` access to the cluster who want to properly populate the `airflow_db` connection in the Airflow UI with the proper credentials, follow our instructions [here](https://www.astronomer.io/docs/ee-administration-postgres-creds/).

> **Note:** This is entirely optional and is not required, as the `AIRFLOW_CONN_AIRFLOW_DB` Environment Variable is pre-set on Astronomer and will allow users to reference the `airflow_db` connection in a DAG even though the connection itself isn't populated in the Airflow UI.

### What's Next

For a list of handy queries to reference, check out [Useful SQL queries for Apache Airflow](https://www.astronomer.io/guides/airflow-queries/).
