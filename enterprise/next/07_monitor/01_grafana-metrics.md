---
title: "Metrics in Astronomer Enterprise"
navTitle: "Metrics"
description: "Get a single-pane view into the health of your deployments with Grafana dashboards."
---

## Overview

On Astronomer, metrics generated by StatsD and the Kubernetes API via Prometheus are funneled into a set of [Grafana](https://grafana.com/) dashboards that allow you to monitor the status of all Airflow Deployments on your cluster.

All dashboards provide real-time, up-to-date information on the status of your Apache Airflow environment both at the cluster and individual container level.

Read the rest of this guide to learn about Astronomer's built-in dashboards, as well as key metrics to track the overall health of your platform.

If you're interested in customizing the pre-built dashboards to create other views or compile additional metrics, skip to [Creating Custom Dashboards](/docs/enterprise/next/monitor/grafana-metrics#creating-custom-dashboards).

## Access Grafana

By default, users with [System Admin permissions](https://www.astronomer.io/docs/enterprise/v0.23/manage-astronomer/manage-platform-users#system-roles) can access Astronomer's built-in Grafana dashboards. To access Grafana dashboards from the Astronomer UI:

1. As an Astronomer Admin user, open Grafana under your user menu.

    ![Create Grafana Dashboard](https://assets2.astronomer.io/main/docs/grafana/ui-access.png)

2. Log in to Grafana. If you're on the Welcome page, you can log in via the **Sign In** button at the bottom of the sidebar menu. The default login credentials are `admin:admin`

3. Click the search button to access all published dashboards.

By default, most dashboards show metrics for all of your Airflow Deployments. To see metrics for a specific Deployment:

1. In the Astronomer UI, go to **Deployments**. Make note of the release name for the Deployment you want to monitor.

2.

## Astronomer UI Metrics Dashboard

Each of your Deployments has a built-in metrics dashboard in the Astronomer UI. To get there, open a Deployment and go to the **Metrics** tab:

![Astronomer Metrics Dashboard](https://assets2.astronomer.io/main/docs/grafana/astro-metrics.png)

This dashboard is most useful for tracking the performance and resource usage of an individual Airflow Deployment, whereas Grafana is most useful for tracking performance at the platform level and across multiple Airflow Deployments.

### Key Metrics

- **Usage Quotas**: These metrics show the percentage of maximum pods, CPU, and memory an Airflow Deployment is using within its Kubernetes namespace. The upper limit of each metric is pulled from your platform-level configurations for resource quotas.

    If you're using the Local or Celery Executors, these metrics should each show around 50% usage at all times.

    If you're using the Kubernetes Executor or the KubernetesPodOperator in your DAGs, these metrics should fluctuate based on the number of tasks you're running at any given time. If any of these numbers reaches its maximum, we recommend allocating more computing power by adjusting the **Extra Capacity** slider in the **Settings** tab of your Airflow Deployment.

## Airflow State

This dashboard includes a birds-eye view of resource usage and system stress levels. Use this dashboard to monitor all Airflow Deployments running within your Kubernetes cluster in a single view.

![Astronomer State](https://assets2.astronomer.io/main/docs/ee/airflow_state.png)

### Key Metrics

- **CPU Requests and Memory Requests:** These metrics appear in the **Quotas** panel of the dashboard. Most organizations run Astronomer within a cloud environment and pay for resource usage, so it is important to monitor this dashboard and its associated costs. Due to the varying structure, tools, and pricing models of cloud provider solutions, the recommended values for these metrics will vary between organizations.

## Kubernetes Pods

This dashboard allows you to explore any Kubernetes pod within your cluster in detail. If you filter by namespace for a particular Airflow Deployment, you can view a dedicated set of metrics for any pod within that namespace.

![Kubernetes Pod Dashboard](https://assets2.astronomer.io/main/docs/grafana/kube-pod.png)

### Key Metrics

**Pod Status:** This metric shows the status of a single pod within your cluster, and displays one of 5 states [as defined by Kubernetes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#) - `Pending`, `Running`, `Succeeded`, `Failed`, or `Unknown`. The value for this metric is equivalent to the output of ` kubectl get pod <pod-name> -n <namespace>`. If you're interested in seeing the status of multiple pods at once, you're free to add a panel to this dashboard.

## Airflow Deployment Overview

Use this Dashboard to drill down to each individual Deployment on your platform.

![Astronomer Deployments](https://assets2.astronomer.io/main/docs/ee/airflow_deployment_overview.png)

### Key metrics

**Deployment Status:** This metric appears in the **At a Glance** panel of the dashboard. It is a single value that indicates whether a Deployment is healthy or unhealthy.

A Deployment's status is determined by whether all pods in an Airflow deployment are running. If any pods aren't running as expected, the Deployment's status becomes "Unhealthy".

## Platform Overview
This will show the amount of persistent storage available to Prometheus, the registry, and ElasticSearch. It will also show a summary of all alerts that have fired.

![Astronomer Platform](https://assets2.astronomer.io/main/docs/ee/platform_overview.png)

### Key metrics

- **1-day Airflow Task Volume:** This metric is the single best tool for monitoring Airflow task successes and failures. It can also be adjusted to better find specific data points.

    For instance, if you click **Edit** in the dropdown menu for the metric, you're able to update the **Metrics** query to show data for a specific time, status, or Deployment.

    ![1-day Airflow Task Volume Metric](https://assets2.astronomer.io/main/docs/grafana/task-volume.png)

    In the following example, the query has been rewritten in the format `sum(increase(airflow_ti_successes[<time>]))`, which changes the time interval that appears in the metric. We recommend looking at trends across longer intervals to clearly identify trends in task failures and successes:

    ![Task Volume Metric with Modified Time Interval](https://assets2.astronomer.io/main/docs/grafana/time-interval.png)

    In addition, the eye icon on the metric can be deselected so that only task failures are shown:

    ![Failures-Only Task Volume Metric](https://assets2.astronomer.io/main/docs/grafana/failures-only.png)

    You could also write this query as `sum by (deployment) (increase(airflow_ti_failures[<time-interval>]))` to view task volume across an entire Deployment, which is useful for monitoring potential system-wide problems:

    ![Metric Across Multiple Deployments](https://assets2.astronomer.io/main/docs/grafana/multiple-deployments.png)

- **Database Connections:** This metric can be found in the **Database** panel. It measures how often your database is reached out to by the Airflow Scheduler, Webserver, and Workers. The chart shows the sum total of connections coming from sqlpooling in all Airflow Deployments in your environment:

    ![Database Connections Metric](https://assets2.astronomer.io/main/docs/grafana/db-connections.png)

    This metric is particularly relevant to organizations with more than 20 Airflow Deployments. For optimal performance, this should stay under 300 less than the `max_connections` for your postgresSQL database.  

- **PG Bouncer Waiting Clients:** This metric can be found in the **Database** panel. It measures how long specific actions are queued in a given Airflow Deployment before being executed. In healthy Deployments, this number should be very low.

    Extended periods of waiting can degrade performance and should be investigated for potential problems. For example, the Deployments associated with the red and pink spikes in the following graph might be experiencing issues with successfully executing tasks:

    ![PG Bouncer metrics](https://assets2.astronomer.io/main/docs/grafana/PGBouncer.png)

- **Unhealthy Schedulers:**  This metric is available in the **Airflow Health** panel, but Scheduler health can also be assessed for each individual Deployment in the Astronomer UI's **Metrics** tab. This metric gives you info on when and for how long individual Airflow deployment schedulers restarted.  

    Scheduler restarts could be normal, such as during an update, but if you see a single scheduler continue to restart or stay in an unhealthy state for a significant amount of time, it is worth investigating further.

    For example, an organization would want to investigate the green Scheduler in the following screenshot:

    ![Unhealthy Schedulers](https://assets2.astronomer.io/main/docs/grafana/unhealthy-shcedulers.png)

- **DAG Parsing Time:** This metric is available in the **Airflow Health** panel. It measures how quickly the Scheduler is executing your DAGs, and it's an indicator for how well your Scheduler is scheduling jobs for execution:

    ![Parsing Time Metric](https://assets2.astronomer.io/main/docs/grafana/parsing-time.png)

   Anything under 1 second is considered good, but the lower the measured time the better. Note that operator executions are not included in this metric, as those are typically scheduled for execution in Worker pods.

- **Elasticsearch Available Disk Space:** Astronomer leverages the ELK stack for logging events; this is an important part of establishing observability for the platform as a whole. To do this successfully, Elasticsearch should always have >20% Available Disk Space. You can monitor this from the “Platform Overview” dashboard to ensure that logs have been successfully captured and persisted:

   ![Available Disk Space](https://assets2.astronomer.io/main/docs/grafana/available-disk-space.png)

   If this ever dips below 20%, we recommend increasing the replica count in the Elasticsearch-data helm chart. The helm changes will look something like the following:

   ```yaml
   elasticsearch:
     data:
       replicas: <number>
   ```

## Fluentd

This dashboard tracks the performance of Fluentd.

![Fluentd Dashboard](https://assets2.astronomer.io/main/docs/grafana/Fluentd.png)

### Key Metrics

- **Buffer Size and Buffer Length:** These metrics track whether the Fluentd buffer is getting backed up, which might indicate an issue with writing logs to Elasticsearch. These metrics should ideally be hovering around zero.

## Creating Custom Dashboards

Because Astronomer's key metrics are distributed across several dashboards, you might want to create a custom dashboard that shows these metrics in one place. To create a custom dashboard:

1. As an Astronomer Admin user, open Grafana from the Astronomer UI.

    ![Create Grafana Dashboard](https://assets2.astronomer.io/main/docs/grafana/ui-access.png)

2. Log in to Grafana. If you're on the Welcome page, you can log in via the **Sign In** button at the bottom of the sidebar menu. The default login credentials are `admin:admin`

2. In the Grafana sidebar menu, click the **+** icon and click **Create Dashboard**.

    ![Create Grafana Dashboard](https://assets2.astronomer.io/main/docs/grafana/create-dashboard.png)

3. Specify a visualization type and a title for your first panel.

4. In the metric's **Query** tab, open the data source dropdown menu and select Prometheus.

5. In the table below the data source dropdown menu. Specify the metric you want to visualize in the **Metrics** field.

6. On the top menu, click the **Add panel** icon to add another panel to the dashboard. Repeat steps 2 and 3 for this panel.

7. Click **Save** to finalize your changes.

As a starting point, we recommend creating a dashboard with the following metrics visualized as graphs for any organizations using the Kubernetes Executor or KubernetesPodOperator in an Airflow Deployment:

- Node Disk Space Utilization
- Node CPU/Memory Utilization
- Disk IO Utilization
- CPU Utilisation
- Memory Utilisation
- CPU Saturation (Load1 per CPU)
- Memory Saturation (Major Page Fails)
- Net Utilisation (Bytes Receive/ Transmit)
- Net Saturation (Drops Receive/Transmit)

These metrics can help you monitor resource usage and subsequent costs of your Kubernetes pods. The recommended steady-state values for these metrics all will be specific to your Kubernetes environment and dependent on available resources/configurations. However, if metrics disappear from this view, it is typically an indication of the node going away and worker pods being relocated to another Kubernetes node.
