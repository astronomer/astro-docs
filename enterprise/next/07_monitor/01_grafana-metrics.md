---
title: "Metrics in Astronomer Enterprise"
navTitle: "Metrics"
description: "Get a single-pane view into the health of your deployments with Grafana dashboards."
---

## Overview

Astronomer takes metrics generated by Statsd, as well as metrics generated from the KubeAPI by Prometheu,s and funnels them into Grafana dashboards where you can see the status of all Airflow deployments on your cluster.

Enterprise admins can access Astronomer's built in Grafana dashboards on `grafana.BASEDOMAIN`. All dashboards provide live, up-to-date information on the status of Airflow, from the Kubernetes level to the container level.

## Astronomer UI Dashboard

Each of your Deployments has a built-in metrics dashboard in the Astronomer UI. To get there, open a Deployment and go to the **Metrics** tab:

![Astronomer Metrics Dashboard](https://assets2.astronomer.io/main/docs/grafana/astro-metrics.png)

This dashboard is most useful for tracking the performance of individual Airflow Deployments, whereas Grafana dashboards are more useful for tracking performance at the platform level.

### Key Metrics

- **Pod / CPU / Memory Usage**: These metrics measure the amount of pods, CPUs, and memory being used by an individual Deployment in its Kubernetes namespace. The upper limits of these metrics are the maximum allowed resources defined in the Kubernetes cluster.

    If you're using the Local or Celery Executors, these metrics should each show around 50% usage at all times.

    If you're using the Kubernetes executor or the KubernetesPodOperator in your DAGs, these metrics should fluctuate in usage based on how many tasks you're running. If these metrics increase to near-full usage when running your tasks, you can allocate more computing power by adjusting the **Extra Capacity** slider in the **Settings** tab for the Deployment.

## Airflow State

Monitor all of your deployments from the Kubernetes level, including a birds-eye view of resource usage and system stress levels. When you spin up new deployments, they'll first show as "Unhealthy" in this view before going to "Healthy" when it is ready to be used.

![Astronomer State](https://assets2.astronomer.io/main/docs/ee/airflow_state.png)

### Key Metrics

- **CPU Requests and Memory Requests:** These metrics appear in the **Quotas** panel of the dashboard. If you are executing your workflows in a cloud infrastructure, the typical paradigm is to pay for the resources you actually use, so it is important to monitor this usage and its associated costs.

    Due to the varying structure, tools, and pricing models of cloud provider solutions, the recommended values for these metrics will vary between organizations.

## Kubernetes Pods

This dashboard shows the status of the Kubernetes pods in your environment.

![Kubernetes Pod Dashboard](https://assets2.astronomer.io/main/docs/grafana/kube-pod.png)

### Key Metrics

**Pod Status:** This metric shows the status of your pods in your environment, which is the same value you get by running `$ kubectl get pod <pod-name> -n <namespace>`. If desired, you can add a panel to the dashboard to easily see states for all pods at once.

## Airflow Deployment Overview

Use this Dashboard to drill down to each individual Deployment on your platform.

![Astronomer Deployments](https://assets2.astronomer.io/main/docs/ee/airflow_deployment_overview.png)

### Key metrics

**Deployment Status:** This metric appears in the **At a Glance** panel of the dashboard. It is a single value that indicates whether a Deployment is healthy or unhealthy.

A Deployment's status is determined by whether all pods in an Airflow deployment are running. If any pods aren't running as expected, the Deployment's status becomes "Unhealthy".


## Platform Overview
This will show the amount of persistent storage available to Prometheus, the registry, and ElasticSearch. It will also show a summary of all alerts that have fired.

![Astronomer Platform](https://assets2.astronomer.io/main/docs/ee/platform_overview.png)

### Key metrics

- **1-day Airflow Task Volume:** This metric is the single best tool for monitoring Airflow task successes and failures. It can also be adjusted to better find specific data points.

    For instance, if you click **Edit** in the dropdown menu for the metric, you're able to update the **Metrics** query to show data for a specific time, status, or Deployment.

    ![1-day Airflow Task Volume Metric](https://assets2.astronomer.io/main/docs/grafana/task-volume.png)

    In the following example, the query has been rewritten in the format `sum(increase(airflow_ti_successes[<time>]))`, which changes the time interval that appears in the metric. We recommend looking at trends across longer intervals to clearly trends in task failures and successes.

    You could also write this query as `sum by (deployment) (increase(airflow_ti_failures[<time-interval>]))` to view task volume across an entire Deployment, which is useful for monitoring potential system-wide problems.

    In addition, the eye icon on the metric has been deselected so that only task failures are shown:

    ![Failures-Only Task Volume Metric](https://assets2.astronomer.io/main/docs/grafana/failures-only.png)

- **Database Connections:** This metric can be found in the **Database** panel. It measures how often your database is reached out to by the Airflow Scheduler, Webserver, and Workers. The chart shows the sum total of connections coming from sqlpooling in all Airflow Deployments in your environment.

    This metric is particularly relevant to organizations with more than 20 Airflow Deployments. For optimal performance, this should stay under 300 less than the `max_connections` for your postgresSQL database.  

- **PG Bouncer Waiting Clients:** This metric can be found in the **Database** panel. It measures how long specific actions are queued in a given Airflow Deployment before being executed. In healthy Deployments, this number should be very low.

    Extended periods of waiting can degrade performance and should be investigated for potential problems. For example, the Deployments associated with the red and pink spikes in the following graph might be experiencing issues with successfully executing tasks:

    ![PG Bouncer metrics](https://assets2.astronomer.io/main/docs/grafana/PGBouncer.png)

- **Unhealthy Schedulers:**  This metric is available in the **Airflow Health** panel, but Scheduler health can also be assessed for each individual Deployment in the Astronomer UI's **Metrics** tab. This metric gives you info on when and for how long individual Airflow deployment schedulers restarted.  

    Scheduler restarts could be normal, such as during an update, but if you see a single scheduler continue to restart or stay in an unhealthy state for a significant amount of time, it is worth investigating further.

    For example, an organization would want to investigate the green Scheduler in the following screenshot:


- **DAG Parsing Time:** This metric is available in the **Airflow Health** panel. It measures how quickly the Scheduler is executing your DAGs, and it's an indicator for how well your Scheduler is scheduling jobs for execution.

   Anything under 1 second is considered good, but the lower the measured time the better. Note that operator executions are not included in this metric, as those are typically scheduled for execution in Worker pods.

- **Elasticsearch Available Disk Space:** Astronomer leverages the ELK stack for logging events; this is an important part of establishing observability for the platform as a whole. To do this successfully, Elasticsearch should always have >20% Available Disk Space which you can monitor from the “Platform Overview” dashboard, to ensure that logs are captured and persisted successfully.

   If this ever dips below 20%, we recommend increasing the replica count in the Elasticsearch-data helm chart. The helm changes will look something like the following:

   ```yaml
   elasticseatch:
     data:
       replicas: <number>
   ```

## Fluentd

This dashboard tracks the performance of Fluentd.

![Fluentd Dashboard](https://assets2.astronomer.io/main/docs/grafana/Fluentd.png)

### Key Metrics

- **Buffer Size and Buffer Length:** These metrics track whether the Fluentd buffer is getting backed up, which might indicate an issue with writing logs to Elasticsearch. These metrics should ideally be hovering around zero.

## Creating Custom Dashboards

Because Astronomer's key metrics are distributed across several dashboards, you might want to create a custom dashboard that shows these metrics in one place. To create a custom dashboard:

1. Ensure you're logged into Grafana. If you're on the Welcome page, you can log in via the **Sign In** button at the bottom of the sidebar menu. The default login credentials are `admin:admin`
2. In the Grafana sidebar menu, click the **+** icon and click **Create Dashboard**.

    ![Create Grafana Dashboard](https://assets2.astronomer.io/main/docs/grafana/create-dashboard.png)

3. Specify a visualization type and a title for your first panel.
4. In the metric's **Query** tab, open the data source dropdown menu and select Prometheus.
5. In the table below the data source dropdown menu. Specify the metric you want to visualize in the **Metrics** field.
6. On the top menu, click the **Add panel** icon to add another panel to the dashboard. Repeat steps 2 and 3 for this panel.
7. Click **Save** to finalize your changes.

As a starting point, we recommend creating a dashboard with the following metrics visualized as graphs for any organizations using the Kubernetes Executor or KubernetesPodOperator in an Airflow Deployment:

- Node Disk Space Utilization
- Node CPU/Memory Utilization
- Disk IO Utilization
- CPU Utilisation
- Memory Utilisation
- CPU Saturation (Load1 per CPU)
- Memory Saturation (Major Page Fails)
- Net Utilisation (Bytes Receive/ Transmit)
- Net Saturation (Drops Receive/Transmit)

These metrics can help you monitor resource usage and subsequent costs of your Kubernetes pods. The recommended steady-state values for these metrics all will be specific to your Kubernetes environment and dependent on available resources/configurations. However, if metrics disappear from this view, it is typically an indication of the node going away and worker pods being relocated to another Kubernetes node.
